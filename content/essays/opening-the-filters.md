---
title: "Opening the Filters"
date: 2026-01-23
description: "Why the obvious hides in plain sight."
tags: ["principles", "perspective", "leadership", "decision-making"]
series: []
ShowToc: true
TocOpen: false
draft: false
---

*Why the obvious hides in plain sight*

The Chinese surveillance balloons floated across American airspace for years before anyone noticed.

Not because we lacked sensors. Not because we lacked data. The U.S. defense system collects more information than any surveillance apparatus in human history. The balloons went undetected because of something far more dangerous than technological failure: we weren't looking for them.

I heard this story recently on the Jordan B. Peterson podcast from Garry Nolan, a Stanford immunologist who's become one of the more credible scientific voices in UAP research. He explained how our defense sensors are programmed to filter for known threat signatures (rockets, planes, missiles). Everything else gets discarded immediately as noise. The system processes what it expects and throws away what it doesn't.

When the UAP research community pushed to "open the filters," to actually look at what was being dumped, the first discovery wasn't extraterrestrial. It was hot-air balloons. Technology from the 1780s, floating through American airspace undetected because no one imagined a threat could be that simple.

The sensors weren't blind. The *interpretation* was blind.

---

## False Completeness

There's a particular kind of blindness that's more dangerous than ignorance.

Ignorance says "I don't know." That's uncomfortable, but it's honest. It leaves the door open for learning.

The dangerous blindness says "I know what there is to know." This is false completeness, the assumption that your categories capture reality, that your filters are correctly calibrated, that anything outside your framework is noise worth discarding.

The defense system designers weren't stupid. They built exactly what they intended: sensors optimized for known threats. The problem was assuming known threats were the only threats. They didn't build a category for "things we haven't imagined yet."

Friedrich Nietzsche saw this clearly over a century ago:

> "There is only a perspective seeing, only a perspective 'knowing'; and the more affects we allow to speak about one thing, the more eyes, different eyes, we can use to observe one thing, the more complete will our 'concept' of this thing, our 'objectivity,' be."

Objectivity doesn't come from having no perspective (that's impossible). It comes from actively seeking *more* perspectives, different perspectives, perspectives that challenge your own. The path to truth runs through the multiplication of viewpoints, not their elimination.

---

## The Structural Solution

Abraham Lincoln understood this problem. In *Team of Rivals*, Doris Kearns Goodwin documents how Lincoln filled his cabinet with political rivals, people who disagreed with him fundamentally. Not because he enjoyed conflict, but because he didn't trust himself to see every angle.

Lincoln built a structure that would *force* opposing viewpoints into the room.

This is the difference between hoping you'll stay open-minded and building systems that guarantee diverse input. Hope fails under pressure. Structure persists.

I try to apply this in my own work. When we form committees or task forces, I often push to include dissenters, people whose perspective might derail the comfortable consensus. The pushback is predictable: "If we include that person, they'll slow everything down."

That objection proves the point. If someone's perspective is so different that it threatens to derail the conversation, that's precisely the perspective most likely to see what everyone else is missing.

The discomfort *is* the signal.

---

## Speed vs. Accuracy

Here's the tension: dissent does slow things down. Including opposing voices creates friction. Opening the filters means processing more data, considering more possibilities, sitting with more uncertainty.

So you have to choose: Are you optimizing for *speed* or for *accuracy*?

In 2026, most people optimize for speed. We jump to conclusions that confirm our worldview. We process information that fits our existing categories and discard the rest. Social media algorithms have trained us to expect instant validation, show me content that matches what I already believe, filter out everything that doesn't.

But the Chinese balloons were hiding in the filtered data. The threats we don't anticipate live in the noise we dismiss. The opportunities we miss are categorized as irrelevant before we ever see them.

Frank Imholte, who spent years as Executive Director of the Minnesota State Auctioneers Association, used to tell me: "There are three sides to every story: his side, her side, and the truth somewhere in between."

Most people camp on their side and argue. Fewer try to understand the other side. Almost nobody goes looking for the third thing, the truth that neither perspective fully holds. That search requires slowing down. It requires admitting your view is incomplete.

---

## The Prerequisite Chain

I've been thinking about what it actually takes to see clearly. It's not a single skill. It's a chain, and every link matters:

1. **Self-confidence**, secure enough to be wrong without feeling diminished
2. **Willingness**, wanting the truth more than wanting to be right
3. **Slowing down**, resisting the pull toward quick conclusions
4. **Listening**, actually processing the dissenting view
5. **Understanding**, sitting with the alternative perspective fully, not just hearing it
6. **Updating**, changing position when the evidence warrants

Break any link and the whole thing fails.

Most people break at step one. It takes genuine confidence to admit you're wrong. Insecure people double down because their identity is fused with their positions. Being wrong feels like being diminished. So they dig in, dismiss the dissenter, optimize for feeling right rather than being accurate.

Confident people can update. They can say "that's a better idea" without it costing them anything, because their sense of self isn't riding on every opinion.

Lincoln could fill his cabinet with rivals because he wasn't threatened by disagreement. The defense system couldn't "admit" its filters were wrong until someone from outside forced the question.

---

## The Sports Analogy

There's a framework I keep coming back to: You're never as good as your best games, but you're never as bad as your worst games. Reality lives in the middle.

This applies to self-assessment, but it also applies to how we evaluate everything. Our highs inflate us; our lows deflate us. The data points we remember most vividly (the wins and the disasters) are usually outliers. The truth lives in the middle, in the boring average, in the patterns we don't notice because they don't trigger strong emotions.

Opening the filters means being suspicious of your own certainties. The things you're most sure about might be the things most worth questioning. The data you're ignoring might contain the signal you need.

---

## Before You Can Apply

I wrote recently about the gap between understanding and doing ([Understanding vs. Doing](/essays/understanding-vs-doing/)), how we collect principles instead of applying them, how knowledge accumulation becomes a sophisticated form of procrastination.

But there's a gap that comes *before* that one. Before you can apply knowledge, you have to be able to see clearly. And before you can see clearly, you have to be willing to be wrong.

This is the prerequisite that most people skip. They assume their perception is accurate and move straight to execution. But if your filters are miscalibrated, if you're only processing information that confirms what you already believe, then all the execution in the world won't help. You'll be efficiently pursuing the wrong targets.

The Mongols understood something about this ([What the Mongols Knew](/essays/what-the-mongols-knew/)). Their religious tolerance wasn't philosophical enlightenment. It was epistemic humility at scale. They didn't care what you believed because they understood that beliefs aren't the same as contributions. They processed the *person*, not the category the person fit into.

Modern ideological conformity does the opposite. We filter people through their stated positions, discard anyone who doesn't match our priors, and wonder why we keep being surprised by reality.

---

## The Real Work

Nietzsche said there are no facts, only interpretations. That's not nihilism. It's a challenge. If every perspective is partial, then the work isn't finding the "right" perspective. The work is multiplying perspectives, seeking out the views that challenge your own, sitting with the discomfort of uncertainty.

The defense system needed someone to say "open the filters." Lincoln needed rivals willing to tell him he was wrong. We all need people and practices that force us outside our comfortable categories.

The answers aren't in better filters. They're in fewer filters. The answers aren't in processing faster. They're in slowing down enough to question what you're not processing.

The obvious hides in plain sight, not because it's invisible, but because we've trained ourselves not to see it.

---

*What are you filtering out? What perspective do you dismiss as noise? What truth is hiding in the data you're discarding?*
